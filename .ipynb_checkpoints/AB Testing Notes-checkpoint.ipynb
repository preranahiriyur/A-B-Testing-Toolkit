{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A/B Testing  \n",
    "\n",
    "This notebook contains my notes on A/B Testing from Udacity's free online course as well as books like Lean Analytics, Lean Product Playbook and Trustworthy Online Experiments: A Practical Guide to A/B Testing (TOE). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition:\n",
    "\n",
    "In simple terms, **A/B tests** or **Online Controlled Experiments** are mechanisms to test new product features, backend changes, UI changes or any strategic decisions by implementing the change only on a subset of users and analyzing the impact on chosen metrics to determine the impact of such a change before officially rolling it out. \n",
    "\n",
    "A/B Tests are best used when introducing enhancements or improvements to products with an existing user base (an available population to conduct tests on). Introduction of new products could benefit more from qualitative methods like consumer interviews and surveys. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some considerations before using A/B Testing on your product:\n",
    "\n",
    "- Do you have an **experimental unit** to test on? - like users, cookies, etc\n",
    "- Do you have enough users to test on? - TOE recommends thousands of experimental units for the experiment to be useful\n",
    "- Do you have a key metric(s), aka **Overall Evaluation Criteria**? - a goal that is agreed upon and can be practically evaluated. \n",
    "- Are the changes you want to test easy to make? - you would need to weigh the costs of making software changes against the benefits potentially realised from the test\n",
    "- For UX testing - do you have repeat users whose experience (in terms of satisfaction, engagement, etc) can be improved/ evaluated? - testing impact of changes on engagement for one-off purchase websites like home rentals may not be ass effective as say a social network or e-commerce site. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Components of an A/B Test:\n",
    "\n",
    "TOE defines the components of an A/B test as follows:\n",
    "\n",
    "- **_Parameter_**: Aka experimental variable/factor. This is what determines the difference between the A and B groups of your test. In case of a simple UI change to the colour of the checkout button, the parameter can be defined as the colour. The original colour is often assigned to group A - known as the **Control Group** and the new colour(s) are assigned the the other groups - known as the **Treatment(s)**. Often times, multiple variations of the parameter are tested in a single test in a process known as **Multivariate testing** \n",
    "\n",
    "\n",
    "- **_Variant_**: This is simply another name for the group assignment - Treatment vs Control or A/B/C... It is a function of the parameter or experimental variable. The distinction between parameter and variant is more apparent when there are 2 or more experimental variables interacting. For example if we are testing for both colour and shape (m and n of each) of a button, we have 2 parameters - colour and shape. We have m times n variants - each combination of colour and shape. \n",
    "\n",
    "\n",
    "- **_Randomization Unit_**: This is also often called the _unit of diversion_. It is the unit on which a group (Treatment or Control) is randomly assigned. The most common unit (especially for user visible changes) is user id. However, for some non user visible (like backend changes) cookie or page view based diversion may also be applied. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "This warrants a whole section to itself because choosing the right metric can change the whole inference and impact of your experiment. \n",
    "\n",
    "- Choosing the right metrics \n",
    "- Normalizing metrics - size may vary across variants \n",
    "- Some common metrics for different business types \n",
    "- Variability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics for Online Controlled Experiments \n",
    "\n",
    "In the this section notebook, I will go through some of the common statistical methods involved in planning and analysing A/B Tests as well as some snippets of code used to obtain results.\n",
    "\n",
    "- Sizing and Power Analysis \n",
    "- Common distributions - Binomial and Normal Distribution - (common metrics and their distributions, a table)\n",
    "- Hypothesis testing - Means vs Proportions \n",
    "- Variability \n",
    "- Empirical vs Analytical estimates and confidence intervals \n",
    "- A/A Tests \n",
    "- Sensitivity vs Robustness \n",
    "- Analyzing Ratios "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample Size Calculations \n",
    "\n",
    "#inputs \n",
    "power <- 80\n",
    "conflevel <- 95\n",
    "baseline <- 8\n",
    "effectsize <- 2\n",
    "relative <- F\n",
    "\n",
    "#function to calculate sample size \n",
    "sample_size <- function(power, conflevel, baseline, effectsize, relative){\n",
    "    if (relative == T){\n",
    "        effectsize <- (effectsize/100)*(baseline)\n",
    "    }\n",
    "    else {\n",
    "        effectsize <- effectsize\n",
    "    }\n",
    "    p1 <- baseline/100\n",
    "    p2 <- p1 - effectsize/100\n",
    "    p <- (p1+p2)/2\n",
    "    q <- 1 - p\n",
    "    q1 <- 1 - p1\n",
    "    q2 <- 1 - p2\n",
    "    alpha <- 1 - conflevel/100\n",
    "    n <- (sqrt(p*q*(2))*qnorm(1-alpha/2) + sqrt(p1*q1 + p2*q2)*qnorm(power/100))^2/(effectsize/100)^2\n",
    "    return(round(n))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "2554"
      ],
      "text/latex": [
       "2554"
      ],
      "text/markdown": [
       "2554"
      ],
      "text/plain": [
       "[1] 2554"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_size(power, conflevel, baseline, effectsize, relative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More information about the formula used here can be found in this [article](https://towardsdatascience.com/required-sample-size-for-a-b-testing-6f6608dd330a) published by Towards Data Science. There are also various online sample size calculators that provide close estimates. \n",
    "\n",
    "- Optimizely \n",
    "- One more calculator link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Designing an Experiment \n",
    "\n",
    "- Unit of diversion - considerations \n",
    "- Size - considerations \n",
    "- Population vs Cohort vs Target Population\n",
    "- Duration vs Exposure - considerations to limit exposure "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ethics of A/B Testing and Online Experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Pitfalls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
